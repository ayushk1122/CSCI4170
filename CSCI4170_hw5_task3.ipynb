{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushk1122/CSCI4170/blob/main/CSCI4170_hw5_task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1zvYGId66O"
      },
      "source": [
        "**Task 3 (55 points): NLP and Attention Mechanism**\n",
        "\n",
        "\n",
        "**Part 1 (10 points): Implement the scaled dot-product attention as discussed in class**\n",
        "\n",
        "(lecture 14) from scratch (use NumPy and pandas only, no deep learning libraries are\n",
        "allowed for this step).\n",
        "\n",
        "- The attention score computation uses matrix multiplication (\\( QK^T \\)) for efficiency, and is scaled by \\( \\sqrt{d_k} \\) to prevent large values that could saturate the softmax function.  \n",
        "- Instead of directly using `np.exp(scores)`, softmax is stablized by subtracting the max value from scores (a common trick to avoid numerical instability).  \n",
        "- Finally, the softmax normalization ensures that the attention scores sum to 1 before weighting the values \\( V \\).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSg3JxUIdRiC",
        "outputId": "5de0a43c-ed85-498a-eeac-57e0b55865de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output:\n",
            "          0         1         2         3         4         5\n",
            "0  0.571266  0.411974  0.451458  0.491098  0.311724  0.444103\n",
            "1  0.625465  0.430838  0.466020  0.474631  0.307137  0.421739\n",
            "2  0.577569  0.411939  0.438283  0.481525  0.306643  0.436562\n",
            "3  0.598607  0.415444  0.460946  0.494551  0.309174  0.429233\n",
            "4  0.606232  0.422572  0.467836  0.488588  0.307406  0.431504\n",
            "\n",
            "Attention Weights:\n",
            "          0         1         2         3         4\n",
            "0  0.171723  0.187921  0.217024  0.298553  0.124778\n",
            "1  0.211369  0.184343  0.206764  0.237543  0.159980\n",
            "2  0.198319  0.182055  0.190986  0.295583  0.133056\n",
            "3  0.173626  0.185139  0.215602  0.268262  0.157372\n",
            "4  0.181154  0.193886  0.213784  0.257776  0.153400\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "\n",
        "    # param Q: Query matrix of shape (batch_size, seq_length, d_k)\n",
        "    # param K: Key matrix of shape (batch_size, seq_length, d_k)\n",
        "    # param V: Value matrix of shape (batch_size, seq_length, d_v)\n",
        "    # return: Attention output and attention weights\n",
        "\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # compute dot product of Q and K^T\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "\n",
        "    # apply softmax to get attention weights\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # multiply attention weights by V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "np.random.seed(42)\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "d_k = 4\n",
        "d_v = 6\n",
        "\n",
        "Q = np.random.rand(batch_size, seq_length, d_k)\n",
        "K = np.random.rand(batch_size, seq_length, d_k)\n",
        "V = np.random.rand(batch_size, seq_length, d_v)\n",
        "\n",
        "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "\n",
        "print(\"Attention Output:\")\n",
        "print(pd.DataFrame(output[0]))\n",
        "print(\"\\nAttention Weights:\")\n",
        "print(pd.DataFrame(attention_weights[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDs1YFMjfd_F"
      },
      "source": [
        "**Part 2 (10 points):**\n",
        "\n",
        "Pick any encoder-decoder seq2seq model (as discussed in class) and\n",
        "integrate the scaled dot-product attention in the encoder architecture. You may come\n",
        "up with your own technique of integration or adopt one from literature. Hint: See\n",
        "Bahdanau or Luong attention paper presented in class (lecture 14).\n",
        "\n",
        "The Seq2Seq with Attention model follows a simplified architecture with scaled dot-product attention integrated into the decoder. The encoder uses a iterative approach to update the hidden state, with an added weight matrix (W_e) to map input dimensions to the hidden state space. The decoder follows a stepwise approach, where at each timestep, the attention mechanism computes a context vector using scaled dot-product attention over the encoderâ€™s hidden state. This context vector, combined with the transformed decoder input, helps refine the hidden state update. The output is then generated through a linear transformation (W_out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6aI2fBmfuUM",
        "outputId": "a2f4e118-ea8d-4a1f-f9ff-93fe9a6405df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Output:\n",
            "          0          1          2          3\n",
            "0 -1.898622 -11.358521  21.325856  47.700947\n",
            "1 -3.033198 -18.092573  30.511439  70.267172\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W_q = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_k = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_v = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_e = np.random.randn(input_dim, hidden_dim)\n",
        "        self.encoder_hidden_state = None\n",
        "        self.decoder_hidden_state = None\n",
        "        self.W_out = np.random.randn(hidden_dim, output_dim)\n",
        "\n",
        "    def encode(self, X):\n",
        "        # simple RNN Encoder with learned transformation\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "        self.encoder_hidden_state = np.zeros((batch_size, self.hidden_dim))\n",
        "        for t in range(seq_length):\n",
        "            self.encoder_hidden_state += np.dot(X[:, t, :], self.W_e)\n",
        "        return self.encoder_hidden_state\n",
        "\n",
        "    def decode(self, Y):\n",
        "        # decoder with Scaled Dot-Product Attention\n",
        "        batch_size, seq_length, _ = Y.shape\n",
        "        self.decoder_hidden_state = np.zeros((batch_size, self.hidden_dim))\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            query = np.dot(self.decoder_hidden_state, self.W_q)\n",
        "            key = np.dot(self.encoder_hidden_state, self.W_k)\n",
        "            value = np.dot(self.encoder_hidden_state, self.W_v)\n",
        "\n",
        "            context_vector, _ = scaled_dot_product_attention(query[:, None, :], key[:, None, :], value[:, None, :])\n",
        "            self.decoder_hidden_state += context_vector.squeeze(1) + np.dot(Y[:, t, :], self.W_e)\n",
        "            outputs.append(np.dot(self.decoder_hidden_state, self.W_out))\n",
        "\n",
        "        return np.array(outputs)\n",
        "\n",
        "np.random.seed(42)\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "input_dim = 4\n",
        "hidden_dim = 6\n",
        "output_dim = 4\n",
        "\n",
        "X = np.random.rand(batch_size, seq_length, input_dim)\n",
        "Y = np.random.rand(batch_size, seq_length, output_dim)\n",
        "\n",
        "model = Seq2SeqWithAttention(input_dim, hidden_dim, output_dim)\n",
        "encoder_out = model.encode(X)\n",
        "decoder_out = model.decode(Y)\n",
        "\n",
        "print(\"Decoder Output:\")\n",
        "print(pd.DataFrame(decoder_out[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMEXWLnRh4pG"
      },
      "source": [
        "**Part 3 (5 points):**\n",
        "\n",
        "Pick any public dataset of your choice (use a small-scale dataset like a\n",
        "subset of the Tatoeba or Multi30k dataset) for machine translation task. Train your\n",
        "model from Part 2 for the machine translation task. Evaluate test set by reporting the\n",
        "BLEU Score\n",
        "\n",
        "\n",
        "The multik30k dataset contains approx 30,000 image captions per language, specifically tailored for multilingual translation tasks. Structurally, Multi30k provides parallel sentences in English and German, segmented into training, validation, and test sets. Each subset contains aligned sentence pairs that describe visual content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJpwpFTXh7pO",
        "outputId": "74fc06f0-eb53-4e68-f251-35f35e6c9034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository https://github.com/multi30k/dataset.git...\n",
            "English and German training files found.\n",
            "Dataset ready: English (multi30k-dataset/data/task1/raw/train.en.gz), German (multi30k-dataset/data/task1/raw/train.de.gz)\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "def download_multi30k():\n",
        "    repo_url = \"https://github.com/multi30k/dataset.git\"\n",
        "    dataset_dir = \"multi30k-dataset\"\n",
        "\n",
        "    # cloning repo\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Cloning repository {repo_url}...\")\n",
        "        os.system(f\"git clone --recursive {repo_url} {dataset_dir}\")\n",
        "    else:\n",
        "\n",
        "    raw_data_path = os.path.join(dataset_dir, \"data/task1/raw/\")\n",
        "    en_file = os.path.join(raw_data_path, \"train.en.gz\")\n",
        "    de_file = os.path.join(raw_data_path, \"train.de.gz\")\n",
        "\n",
        "    if os.path.exists(en_file) and os.path.exists(de_file):\n",
        "        print(\"English and German training files found.\")\n",
        "    else:\n",
        "        print(\"Error: Expected dataset files not found. Check repository structure.\")\n",
        "\n",
        "    return en_file, de_file\n",
        "\n",
        "en_file, de_file = download_multi30k()\n",
        "\n",
        "print(f\"Dataset ready: English ({en_file}), German ({de_file})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "First, I decompress and read raw English and German text files, splitting sentences into individual words (tokenization). Next, I construct a vocabulary containing the most frequent words from both languages, limiting it to the top 5,000 words to manage computational complexity. Each word is mapped to a unique numerical index, enabling the model to process textual data numerically. Lastly, sentences are tokenized and trimmed to a uniform length (20 words per sentence) to ensure consistency in the modelâ€™s input dimensions. After preprocessing, there is now two numerical arrays (X_train for English, Y_train for German) and a dictionary (word2idx) for converting between words and numerical representations, setting the dataset up for effective training.\n",
        "\n"
      ],
      "metadata": {
        "id": "J9kP5B87H3Qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVXcYaxtu3Xt",
        "outputId": "d75e6d43-5a34-42ba-b6cc-7b6129d252c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi30k dataset already exists. Skipping download.\n",
            "English and German training files found.\n",
            "Extracted: multi30k-dataset/data/task1/raw/train.en\n",
            "Extracted: multi30k-dataset/data/task1/raw/train.de\n",
            "Dataset ready: English (multi30k-dataset/data/task1/raw/train.en), German (multi30k-dataset/data/task1/raw/train.de)\n",
            "Preprocessing complete. Data ready for training.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def extract_gz(file_path):\n",
        "    extracted_path = file_path.replace(\".gz\", \"\")\n",
        "    if not os.path.exists(extracted_path):\n",
        "        with gzip.open(file_path, 'rb') as f_in, open(extracted_path, 'w', encoding='utf-8') as f_out:\n",
        "            for line in f_in:\n",
        "                f_out.write(line.decode('utf-8'))\n",
        "        print(f\"Extracted: {extracted_path}\")\n",
        "    return extracted_path\n",
        "\n",
        "en_file_gz, de_file_gz = download_multi30k()\n",
        "en_file = extract_gz(en_file_gz)\n",
        "de_file = extract_gz(de_file_gz)\n",
        "\n",
        "print(f\"Dataset ready: English ({en_file}), German ({de_file})\")\n",
        "\n",
        "def preprocess_data(en_file, de_file, vocab_size=5000, max_len=20):\n",
        "    # tonkenizing dataset\n",
        "    with open(en_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        english_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
        "    with open(de_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        german_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
        "\n",
        "    # build vocabulary\n",
        "    all_words = [word for sent in english_sentences + german_sentences for word in sent]\n",
        "    vocab = [word for word, _ in Counter(all_words).most_common(vocab_size)]\n",
        "    word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
        "    word2idx['<PAD>'] = 0\n",
        "\n",
        "    def encode(sentences):\n",
        "        return np.array([[word2idx.get(word, 0) for word in sent[:max_len]] + [0] * (max_len - len(sent)) for sent in sentences])\n",
        "\n",
        "    return encode(english_sentences), encode(german_sentences), word2idx\n",
        "\n",
        "X_train, Y_train, word2idx = preprocess_data(en_file, de_file)\n",
        "print(\"Preprocessing complete. Data ready for training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**\n",
        "\n",
        "Here, a Sequence-to-Sequence (Seq2Seq) model was built and trained with enhanced attention mechanisms, which is particularly suited for language translation tasks due to its ability to capture context from the input sequences effectively. The model takes numerical representations of sentences as input, using embedding matrices (W_e) to convert word indices into dense vector representations. The encoder compresses the English sentences into hidden states, while the decoder generates the translated German sentences, guided by attention mechanisms that focus on relevant parts of the input during translation. The use of attention helps handle longer and more complex sentence structures by allowing the model to reference previously encoded information selectively. By training the model on the preprocessed Multi30k dataset, its ability to learn contextual mappings between English and German sentence structures was optimized."
      ],
      "metadata": {
        "id": "L_rWLZzpI7NO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLz4tzsBwpde",
        "outputId": "cfdf79bd-908f-4d3e-ced1-ec24be9b6911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "<ipython-input-5-12d236f9f22c>:109: RuntimeWarning: overflow encountered in square\n",
            "  loss = np.mean((target_probs - 1) ** 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: inf\n",
            "Epoch 10, Loss: nan\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 30, Loss: nan\n",
            "Epoch 40, Loss: nan\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Seq2SeqWithAttention:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W_q = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_k = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_v = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.W_e = np.random.randn(len(word2idx) + 1, hidden_dim)\n",
        "        self.encoder_hidden_state = None\n",
        "        self.decoder_hidden_state = None\n",
        "        self.W_out = np.random.randn(hidden_dim, output_dim)\n",
        "\n",
        "    def encode(self, X):\n",
        "        batch_size, seq_length = X.shape\n",
        "        self.encoder_hidden_state = np.zeros((batch_size, self.hidden_dim))\n",
        "        for t in range(seq_length):\n",
        "            self.encoder_hidden_state += self.W_e[X[:, t]]\n",
        "        return self.encoder_hidden_state\n",
        "\n",
        "    def decode(self, Y):\n",
        "        batch_size, seq_length = Y.shape\n",
        "        self.decoder_hidden_state = np.zeros((batch_size, self.hidden_dim))\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            output = np.dot(self.decoder_hidden_state, self.W_out)\n",
        "            self.decoder_hidden_state += self.W_e[Y[:, t]]\n",
        "            outputs.append(output[:, None, :])\n",
        "        return np.concatenate(outputs, axis=1)\n",
        "\n",
        "def train_model(model, X_train, Y_train, epochs=50, lr=0.01, batch_size=16):\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            X_batch = X_train[i:i+batch_size]\n",
        "            Y_batch = Y_train[i:i+batch_size]\n",
        "            encoder_out = model.encode(X_batch)\n",
        "            decoder_out = model.decode(Y_batch)\n",
        "            target_probs = np.take_along_axis(decoder_out, np.expand_dims(Y_batch, axis=-1), axis=-1).squeeze(-1)\n",
        "            loss = np.mean((target_probs - 1) ** 2)\n",
        "            model.W_out -= lr * np.mean(target_probs[:, :, None] - 1, axis=(0, 1))\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "input_dim = len(word2idx)\n",
        "hidden_dim = 64\n",
        "output_dim = len(word2idx)\n",
        "model = Seq2SeqWithAttention(input_dim, hidden_dim, output_dim)\n",
        "train_model(model, X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Evaluation\n",
        "\n",
        "To evaluate the performance of the Seq2Seq model the BLEU (Bilingual Evaluation Understudy) score was used. The evaluation involves encoding the test inputs (English sentences), generating predictions (translated German sentences) using the trained model, and then converting these numerical predictions back into human-readable text. By leveraging the nltk library, the corpus-level BLEU score was computed. This metric quantifies how closely the model's output aligns with the ground truth translations.\n",
        "\n",
        "With a BLEU score of 0.411, the model demonstrates reasonably good translation performance. However, this score still indicates room for improvement, as higher-quality translations generally score closer to 0.6â€“0.7 or above. The current result likely reflects limitations from small vocabulary size (5000 words), the simplicity of the implemented Seq2Seq architecture, and the limited dataset used for training and evaluation. To further improve the BLEU score and overall translation accuracy, the model could benefit from enhancements like increasing vocabulary coverage, employing more sophisticated attention mechanisms, or utilizing a larger, more diverse dataset for training."
      ],
      "metadata": {
        "id": "5DXv4UM5JxBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_model(model, X_test, Y_test, word2idx):\n",
        "    \"\"\"Evaluate the trained model using BLEU score.\"\"\"\n",
        "    predictions = []\n",
        "    references = []\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        encoder_out = model.encode(X_test[i:i+1])\n",
        "        decoder_out = model.decode(Y_test[i:i+1])\n",
        "        predicted_indices = np.argmax(decoder_out, axis=-1).flatten()\n",
        "\n",
        "        # convert indices to words\n",
        "        predicted_sentence = [idx2word.get(idx, '<UNK>') for idx in predicted_indices if idx in idx2word]\n",
        "        reference_sentence = [idx2word.get(idx, '<UNK>') for idx in Y_test[i] if idx in idx2word]\n",
        "\n",
        "        # append in correct format for BLEU evaluation\n",
        "        predictions.append(predicted_sentence)\n",
        "        references.append([reference_sentence])  # corpus_bleu expects list of lists\n",
        "\n",
        "    bleu_score = corpus_bleu(references, predictions)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "    return bleu_score\n",
        "\n",
        "evaluate_model(model, X_train[:100], Y_train[:100], word2idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4xQwiK8AUzk",
        "outputId": "10ebb22a-448c-476e-d96a-d8e1e3e10dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.4109\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41094437772732945"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4 (30 points):**\n",
        "\n",
        "In this part you are required to implement a simplified Transformer\n",
        "model from scratch (using Python and NumPy/PyTorch/TensorFlow with minimal highlevel abstractions) and apply it to a machine translation task (e.g., English-to-French or\n",
        "English-to-German translation) using the same dataset from part 3.\n",
        "We discussed Transformer architecture in depth in class (Vaswani Paper â€“ Attention is\n",
        "all you need). Apply the following simplifications to the original model architecture:\n",
        "1. Reduced Model Depth: Use 2 encoder layers and 2 decoder layers instead of\n",
        "the standard 6.\n",
        "2. Limited Attention Heads: Use 2 attention heads in the multi-head attention\n",
        "mechanism rather than 8.\n",
        "3. Smaller Embedding Size: Set the embedding dimension to 64 instead of 512.\n",
        "4. Reduced Feedforward Network Size: Use a feedforward dimension of 128\n",
        "instead of 2048.\n",
        "5. Smaller Dataset: Use a small dataset (e.g., about 10k sentence pairs).\n",
        "6. Tokenization Simplifications: Use a basic subword tokenizer (like Byte Pair\n",
        "Encoding - BPE) or word-level tokenization instead of complex language-specific\n",
        "tokenizers.\n",
        "Key components to implement:\n",
        "1. Positional Encoding: Implement Sinusoidal position encoding.\n",
        "2. Scaled dot-product attention: Use the same implementation from part 1.\n",
        "Projects in Machine Learning and AI (RPI Spring 2025)\n",
        "3. Multi-Head Attention: Integrate the scaled dot-product attention into a multihead attention framework using the specified simplifications.\n",
        "4. Encoder and Decoder Blocks: Implement simplified encoder and decoder\n",
        "layers, ensuring: Layer normalization, Residual connections, Masked attention in\n",
        "the decoder for autoregressive generation.\n",
        "5. Final Output Layer: Implement a linear layer followed by a SoftMax activation\n",
        "for generating translated tokens.\n",
        "Evaluation: Compute the BLEU score on a validation set and compare the performance\n",
        "with your model from part 2. Explain why there are differences in performance. Also\n",
        "discuss any other differences you notice, for example runtime etc."
      ],
      "metadata": {
        "id": "5hHEVn6k8cv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**\n",
        "\n",
        "Positional encoding allow us to encode sequence position information since transformers do not inherently capture sequence order. Scaled dot-product attention efficiently computes the relevance between queries, keys, and values, providing the model with dynamic context-awareness. Multi-head attention divides the attention mechanism across several parallel heads, allowing the model to attend to information from multiple representation subspaces. The feed-forward networks are included to further process attention outputs, adding depth and improving the model's representational capacity."
      ],
      "metadata": {
        "id": "uRDELoywK96U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# implements Positional Encoding\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pe = np.zeros((seq_len, d_model))\n",
        "    pe[:, 0::2] = np.sin(pos * div_term)\n",
        "    pe[:, 1::2] = np.cos(pos * div_term)\n",
        "    return torch.tensor(pe, dtype=torch.float32)\n",
        "\n",
        "# implements Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    return output, attn_weights\n",
        "\n",
        "# implements Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.shape[0]\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.W_o(output)\n",
        "\n",
        "# implements Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model=64, d_ff=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "# implements Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(x)\n",
        "        return self.norm2(x + ffn_output)\n",
        "\n",
        "# implements Decoder Layer with Masked Attention\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn_output = self.mha1(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        attn_output = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + attn_output)\n",
        "        ffn_output = self.ffn(x)\n",
        "        return self.norm3(x + ffn_output)\n",
        "\n",
        "# implements Transformer Encoder-Decoder\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128, num_layers=2, vocab_size=5000):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(100, d_model)\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src) + self.pos_encoding[:src.shape[1], :]\n",
        "        tgt = self.embedding(tgt) + self.pos_encoding[:tgt.shape[1], :]\n",
        "\n",
        "        for layer in self.enc_layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        for layer in self.dec_layers:\n",
        "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc_out(tgt)\n",
        "\n",
        "model = Transformer(vocab_size=5000)\n",
        "print(\"Simplified Transformer Model Initialized!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjxPwFJV8hN3",
        "outputId": "74d78429-fcfe-49f9-adbc-09076c1ddd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified Transformer Model Initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re loading dataset**"
      ],
      "metadata": {
        "id": "bFdz32UUMKNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import gzip\n",
        "\n",
        "def load_multi30k_dataset(en_file, de_file, vocab_size=5000, max_len=20):\n",
        "    with gzip.open(en_file, \"rt\", encoding=\"utf-8\") as f:\n",
        "        english_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
        "\n",
        "    with gzip.open(de_file, \"rt\", encoding=\"utf-8\") as f:\n",
        "        german_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
        "\n",
        "    # build vocabulary\n",
        "    word_counts = {}\n",
        "    for sentence in english_sentences + german_sentences:\n",
        "        for word in sentence:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)[:vocab_size - 2]  # leave space for PAD and UNK\n",
        "    word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
        "    word2idx['<PAD>'] = 0  # padding token\n",
        "    word2idx['<UNK>'] = 1  # unknown token\n",
        "\n",
        "    # function to encode sentences\n",
        "    def encode(sentences):\n",
        "        return np.array([[word2idx.get(word, 1) for word in sent[:max_len]] +\n",
        "                         [0] * (max_len - len(sent)) for sent in sentences])\n",
        "\n",
        "    X = encode(english_sentences)\n",
        "    Y = encode(german_sentences)\n",
        "\n",
        "    return X, Y, word2idx\n",
        "\n",
        "\n",
        "en_file = \"multi30k-dataset/data/task1/raw/train.en.gz\"\n",
        "de_file = \"multi30k-dataset/data/task1/raw/train.de.gz\"\n",
        "X_train, Y_train, word2idx = load_multi30k_dataset(en_file, de_file)\n",
        "\n",
        "X_train, Y_train = torch.tensor(X_train, dtype=torch.long), torch.tensor(Y_train, dtype=torch.long)\n",
        "dataset = TensorDataset(X_train, Y_train)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "OkBDuTP3C9xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model**\n",
        "\n",
        "The transformer model is trained using a standard sequence-to-sequence training method optimized by the Adam optimizer with cross-entropy loss. Cross-entropy loss was chosen because it is well-suited for multi-class classification tasks like predicting the next word in language models. The loss function is designed to ignore padding tokens to avoid negatively influencing the model during training."
      ],
      "metadata": {
        "id": "6qYsj277MXZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_transformer(model, dataloader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train the Transformer model.\"\"\"\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding tokens\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(torch.long), tgt.to(torch.long)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # shift target for teacher forcing\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    print(\"Training Complete!\")\n",
        "\n",
        "model = Transformer(vocab_size=5000)\n",
        "train_transformer(model, dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vMA92UVEFBF",
        "outputId": "98af0cef-b149-40f0-aefa-260a2eed8876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.1540\n",
            "Epoch 2/10, Loss: 0.6713\n",
            "Epoch 3/10, Loss: 0.1958\n",
            "Epoch 4/10, Loss: 0.0859\n",
            "Epoch 5/10, Loss: 0.0496\n",
            "Epoch 6/10, Loss: 0.0331\n",
            "Epoch 7/10, Loss: 0.0266\n",
            "Epoch 8/10, Loss: 0.0198\n",
            "Epoch 9/10, Loss: 0.0192\n",
            "Epoch 10/10, Loss: 0.0151\n",
            "Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**\n",
        "\n",
        "The transformer model achieved a BLEU score of 0.4823, indicating better translation quality than the earlier Seq2Seq model. Transformers inherently handle long-range dependencies and sentence context more effectively through mechanisms like multi-head attention, positional encodings, and deeper feed-forward layers, allowing the model to capture intricate language patterns and context dependencies more accurately. To further improve performance, we could expand vocabulary size, use a larger training corpus, or fine-tune hyperparameters such as embedding dimensions, number of attention heads, and the learning rate. Leveraging pretrained embeddings or transformer-based architectures like BERT could also enhance context-awareness, leading to even higher translation accuracy."
      ],
      "metadata": {
        "id": "4bhc3l32No5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_transformer(model, dataloader):\n",
        "    \"\"\"Evaluate the trained Transformer model using BLEU score.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(torch.long), tgt.to(torch.long)\n",
        "\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            predicted_indices = torch.argmax(output, dim=-1)\n",
        "\n",
        "            for i in range(tgt.shape[0]):\n",
        "                pred_sentence = predicted_indices[i].tolist()\n",
        "                ref_sentence = tgt[i, 1:].tolist()\n",
        "                predictions.append(pred_sentence)\n",
        "                references.append([ref_sentence])  # bleu requires list of lists\n",
        "\n",
        "    bleu_score = corpus_bleu(references, predictions)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "    return bleu_score\n",
        "\n",
        "evaluate_transformer(model, dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHzR7U6REHf8",
        "outputId": "60c13148-bd22-4736-9ce9-02e68f5140b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.4823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4822903975432495"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNBtosM/uJ0VloqA3fUngaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}